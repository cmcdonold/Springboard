{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise will require you to pull some data from the Qunadl API. Qaundl is currently the most widely used aggregator of financial market data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, you will need to register a free account on the http://www.quandl.com website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you register, you will be provided with a unique API key, that you should store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the API key as a string - according to PEP8, constants are always named in all upper case\n",
    "API_KEY = '3hwxYeVgT7hmBoiXwjcx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qaundl has a large number of data sources, but, unfortunately, most of them require a Premium subscription. Still, there are also a good number of free datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this mini project, we will focus on equities data from the Frankfurt Stock Exhange (FSE), which is available for free. We'll try and analyze the stock prices of a company called Carl Zeiss Meditec, which manufactures tools for eye examinations, as well as medical lasers for laser eye surgery: https://www.zeiss.com/meditec/int/home.html. The company is listed under the stock ticker AFX_X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the detailed Quandl API instructions here: https://docs.quandl.com/docs/time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is a dedicated Python package for connecting to the Quandl API, we would prefer that you use the *requests* package, which can be easily downloaded using *pip* or *conda*. You can find the documentation for the package here: http://docs.python-requests.org/en/master/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, apart from the *requests* package, you are encouraged to not use any third party Python packages, such as *pandas*, and instead focus on what's available in the Python Standard Library (the *collections* module might come in handy: https://pymotw.com/3/collections/).\n",
    "Also, since you won't have access to DataFrames, you are encouraged to us Python's native data structures - preferably dictionaries, though some questions can also be answered using lists.\n",
    "You can read more on these data structures here: https://docs.python.org/3/tutorial/datastructures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the JSON responses you will be getting from the API map almost one-to-one to Python's dictionaries. Unfortunately, they can be very nested, so make sure you read up on indexing dictionaries in the documentation provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import the relevant modules\n",
    "import requests\n",
    "import json\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, call the Quandl API and pull out a small sample of the data (only one day) to get a glimpse\n",
    "# into the JSON structure that will be returned\n",
    "url_sample = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2018-08-08&end_date=2018-08-14&api_key=3hwxYeVgT7hmBoiXwjcx'\n",
    "r_sample = requests.get(url_sample)\n",
    "r_sample.text\n",
    "sample = r_sample.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dataset'])\n",
      "dict_keys(['id', 'dataset_code', 'database_code', 'name', 'description', 'refreshed_at', 'newest_available_date', 'oldest_available_date', 'column_names', 'frequency', 'type', 'premium', 'limit', 'transform', 'column_index', 'start_date', 'end_date', 'data', 'collapse', 'order', 'database_id'])\n",
      "['Date', 'Open', 'High', 'Low', 'Close', 'Change', 'Traded Volume', 'Turnover', 'Last Price of the Day', 'Daily Traded Units', 'Daily Turnover']\n",
      "[['2018-08-14', None, 74.0, 72.25, 73.0, None, 138029.0, 10067922.0, None, None, None], ['2018-08-13', None, 73.95, 71.1, 73.8, None, 93352.0, 6792143.0, None, None, None], ['2018-08-10', None, 71.75, 68.85, 71.25, None, 83816.0, 5939899.0, None, None, None], ['2018-08-09', None, 69.35, 68.65, 69.35, None, 58850.0, 4068455.0, None, None, None], ['2018-08-08', None, 68.75, 67.7, 68.65, None, 47181.0, 3225473.0, None, None, None]]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the JSON structure of the object you created, and take note of how nested it is,\n",
    "# as well as the overall structure\n",
    "print(sample.keys())\n",
    "d = sample['dataset']\n",
    "print(d.keys())\n",
    "\n",
    "col_names = d['column_names']\n",
    "data_sample = d['data']\n",
    "print(col_names)\n",
    "print(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are your tasks for this mini project:\n",
    "\n",
    "1. Collect data from the Franfurt Stock Exchange, for the ticker AFX_X, for the whole year 2017 (keep in mind that the date format is YYYY-MM-DD).\n",
    "2. Convert the returned JSON object into a Python dictionary.\n",
    "3. Calculate what the highest and lowest opening prices were for the stock in this period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest opening price was 34.0\n",
      "The highest opening price was 53.11\n"
     ]
    }
   ],
   "source": [
    "# Collect data from the FSE for ticker AFX_X for 2017 (Task 1)\n",
    "url = 'https://www.quandl.com/api/v3/datasets/FSE/AFX_X.json?start_date=2017-01-01&end_date=2017-12-31&api_key=3hwxYeVgT7hmBoiXwjcx'\n",
    "r = requests.get(url)\n",
    "#Convert returned json object into a dictionary (Task 2)\n",
    "data = r.json()\n",
    "\n",
    "#Calculate highest and lowest opening prices (Task 3)\n",
    "open_index = data['dataset']['column_names'].index('Open')\n",
    "open_prices = []\n",
    "for day in data['dataset']['data']:\n",
    "    #filter out empty values\n",
    "    if day[open_index] != None:\n",
    "        open_prices.append(day[open_index])\n",
    "max_open = max(open_prices)\n",
    "min_open = min(open_prices)\n",
    "print('The lowest opening price was ' + str(min_open))\n",
    "print('The highest opening price was ' + str(max_open))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What was the largest change in any one day (based on High and Low price)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest change in any one day was 2.8100000000000023\n"
     ]
    }
   ],
   "source": [
    "#Determine largest change in any one day based on High and Low price (Task 4)\n",
    "high_index = data['dataset']['column_names'].index('High')\n",
    "low_index = data['dataset']['column_names'].index('Low')\n",
    "change_within_day = []\n",
    "for day in data['dataset']['data']: \n",
    "    high = day[high_index]\n",
    "    low = day[low_index]\n",
    "    if high != None:\n",
    "        if low != None:\n",
    "            change = abs(low - high)\n",
    "    change_within_day.append(change)\n",
    "max_change_within_day = max(change_within_day)\n",
    "print('The largest change in any one day was ' + str(max_change_within_day))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What was the largest change between any two days (based on Closing Price)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest change between any two days was 2.559999999999995\n"
     ]
    }
   ],
   "source": [
    "#Determine the largest change between days based on closing price (Task 5)\n",
    "close_index = data['dataset']['column_names'].index('Close')\n",
    "close_prices = []\n",
    "for day in data['dataset']['data']: \n",
    "    if day[close_index] != None:\n",
    "        close_prices.append(day[close_index])\n",
    "close_pairs = list(zip(close_prices[:-1], close_prices[1:]))\n",
    "change_between_days = [abs(close1 - close2) for close1, close2 in close_pairs]\n",
    "max_change_between_days = max(change_between_days)\n",
    "print('The largest change between any two days was ' + str(max_change_between_days))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What was the average daily trading volume during this year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average daily trading volume for 2017 was 89124.33725490196\n"
     ]
    }
   ],
   "source": [
    "#Determine average daily trading volume (Task 6)\n",
    "volume_index = data['dataset']['column_names'].index('Traded Volume')\n",
    "traded_volumes = []\n",
    "for day in data['dataset']['data']: \n",
    "    if day[volume_index] != None:\n",
    "        traded_volumes.append(day[volume_index])\n",
    "avg_volume = sum(traded_volumes)/len(traded_volumes)\n",
    "print('The average daily trading volume for 2017 was ' + str(avg_volume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. (Optional) What was the median trading volume during this year? (Note: you may need to implement your own function for calculating the median.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The median trading volume for 2017 was 74975.0\n"
     ]
    }
   ],
   "source": [
    "#Determine median trading volume during the year (Task 7)\n",
    "traded_volumes.sort()\n",
    "median_index = int((len(traded_volumes)/2)-1)\n",
    "median_volume = traded_volumes[median_index]\n",
    "print('The median trading volume for 2017 was ' + str(median_volume))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
