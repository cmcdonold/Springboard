{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I am working with is pre-split into two files, one that contains the training dataset and one that contains the independent dataset. Up to this point, I have been working exclusively with the training dataset for exploratory data analysis and statistical inference. However, rather than using only the training set to train various classifiers, I have instead decided to combine the training and independent datasets into one dataframe so that I can then create my own train/test splits on the data forhyperparameter tuning, cross-validation and assessment. The following table shows the first five rows of the resulting combined dataframe of the raw gene expression data, with the individual genes as the columns and the patient samples as the rows. Overall, there are 7129 genes and 72 samples; however, I will only be working with a small subset of the genes for the subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set plot defaults\n",
    "sns.set()                       # sets default plot style\n",
    "pd.set_option('display.max_columns', 50)\n",
    "sns.set_context(\"paper\")        # sets default plot context\n",
    "plt.rc('figure', figsize=(16,8))# sets default figure size\n",
    "plt.rc('font', size=16)         # controls default text sizes\n",
    "plt.rc('axes', titlesize=18)    # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=16)    # fontsize of the x and y labels\n",
    "plt.rc(['xtick', 'ytick'], labelsize=12)   # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=16)   # fontsize of the legend labels\n",
    "\n",
    "# Read the training dataset into a pandas DataFrame from the 'data_set_ALL_AML_train.csv' file\n",
    "file = 'data_set_ALL_AML_train.csv'\n",
    "data_train = pd.read_csv(file, index_col=1)\n",
    "key = pd.read_csv('actual.csv', index_col=0)\n",
    "\n",
    "# Remove 'call' columns from DataFrame\n",
    "drop_list = ['call'] + ['call.' + str(x) for x in range(1,38)]\n",
    "data_train = data_train.drop(columns=drop_list)\n",
    "\n",
    "# Remove microarray controls (indicated by 'AFFX') from DataFrame rows\n",
    "controls = []\n",
    "for entry in data_train.index:\n",
    "    if 'AFFX' in entry:\n",
    "        controls.append(entry)\n",
    "controls.append('hum_alu_at')\n",
    "data_train = data_train.drop(labels=controls, axis=0)\n",
    "\n",
    "# Drop the 'Gene Description' column as it is not useful here\n",
    "data_train = data_train.drop(['Gene Description'], axis=1)\n",
    "\n",
    "# Map the column entries to integers and sort the index\n",
    "data_train.columns = data_train.columns.map(int)\n",
    "data_train.sort_index(axis=1, inplace=True)\n",
    "\n",
    "# Transpose rows and columns so that each column is a different gene\n",
    "gene_data_train = data_train.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Gene Accession Number</th>\n",
       "      <th>A28102_at</th>\n",
       "      <th>AB000114_at</th>\n",
       "      <th>AB000115_at</th>\n",
       "      <th>AB000220_at</th>\n",
       "      <th>AB000409_at</th>\n",
       "      <th>AB000449_at</th>\n",
       "      <th>AB000450_at</th>\n",
       "      <th>AB000460_at</th>\n",
       "      <th>AB000462_at</th>\n",
       "      <th>AB000464_at</th>\n",
       "      <th>AB000466_at</th>\n",
       "      <th>AB000467_at</th>\n",
       "      <th>AB000468_at</th>\n",
       "      <th>AB000584_at</th>\n",
       "      <th>AB000895_at</th>\n",
       "      <th>AB000896_at</th>\n",
       "      <th>AB000897_at</th>\n",
       "      <th>AB000905_at</th>\n",
       "      <th>AB001106_at</th>\n",
       "      <th>AB001325_at</th>\n",
       "      <th>AB002314_at</th>\n",
       "      <th>AB002315_at</th>\n",
       "      <th>AB002318_at</th>\n",
       "      <th>AB002365_at</th>\n",
       "      <th>AB002366_at</th>\n",
       "      <th>...</th>\n",
       "      <th>Z34822_f_at</th>\n",
       "      <th>U87593_f_at</th>\n",
       "      <th>U88902_cds1_f_at</th>\n",
       "      <th>AC002076_cds2_at</th>\n",
       "      <th>D64015_at</th>\n",
       "      <th>HG2510-HT2606_at</th>\n",
       "      <th>L10717_at</th>\n",
       "      <th>L34355_at</th>\n",
       "      <th>L78833_cds4_at</th>\n",
       "      <th>M13981_at</th>\n",
       "      <th>M21064_at</th>\n",
       "      <th>M93143_at</th>\n",
       "      <th>S78825_at</th>\n",
       "      <th>U11863_at</th>\n",
       "      <th>U29175_at</th>\n",
       "      <th>U48730_at</th>\n",
       "      <th>U58516_at</th>\n",
       "      <th>U73738_at</th>\n",
       "      <th>X06956_at</th>\n",
       "      <th>X16699_at</th>\n",
       "      <th>X83863_at</th>\n",
       "      <th>Z17240_at</th>\n",
       "      <th>L49218_f_at</th>\n",
       "      <th>M71243_f_at</th>\n",
       "      <th>Z78285_f_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151</td>\n",
       "      <td>72</td>\n",
       "      <td>281</td>\n",
       "      <td>36</td>\n",
       "      <td>-299</td>\n",
       "      <td>57</td>\n",
       "      <td>186</td>\n",
       "      <td>1647</td>\n",
       "      <td>137</td>\n",
       "      <td>803</td>\n",
       "      <td>-894</td>\n",
       "      <td>-632</td>\n",
       "      <td>378</td>\n",
       "      <td>-26</td>\n",
       "      <td>-691</td>\n",
       "      <td>2</td>\n",
       "      <td>-156</td>\n",
       "      <td>155</td>\n",
       "      <td>355</td>\n",
       "      <td>1149</td>\n",
       "      <td>-131</td>\n",
       "      <td>158</td>\n",
       "      <td>1084</td>\n",
       "      <td>87</td>\n",
       "      <td>125</td>\n",
       "      <td>...</td>\n",
       "      <td>-325</td>\n",
       "      <td>-67</td>\n",
       "      <td>346</td>\n",
       "      <td>-68</td>\n",
       "      <td>229</td>\n",
       "      <td>-14</td>\n",
       "      <td>108</td>\n",
       "      <td>28</td>\n",
       "      <td>349</td>\n",
       "      <td>61</td>\n",
       "      <td>273</td>\n",
       "      <td>384</td>\n",
       "      <td>-306</td>\n",
       "      <td>-1827</td>\n",
       "      <td>1582</td>\n",
       "      <td>185</td>\n",
       "      <td>511</td>\n",
       "      <td>-125</td>\n",
       "      <td>389</td>\n",
       "      <td>-37</td>\n",
       "      <td>793</td>\n",
       "      <td>329</td>\n",
       "      <td>36</td>\n",
       "      <td>191</td>\n",
       "      <td>-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>263</td>\n",
       "      <td>21</td>\n",
       "      <td>250</td>\n",
       "      <td>43</td>\n",
       "      <td>-103</td>\n",
       "      <td>169</td>\n",
       "      <td>219</td>\n",
       "      <td>2043</td>\n",
       "      <td>188</td>\n",
       "      <td>756</td>\n",
       "      <td>-812</td>\n",
       "      <td>-700</td>\n",
       "      <td>249</td>\n",
       "      <td>-242</td>\n",
       "      <td>-369</td>\n",
       "      <td>-14</td>\n",
       "      <td>-98</td>\n",
       "      <td>131</td>\n",
       "      <td>431</td>\n",
       "      <td>941</td>\n",
       "      <td>-95</td>\n",
       "      <td>328</td>\n",
       "      <td>1215</td>\n",
       "      <td>53</td>\n",
       "      <td>-81</td>\n",
       "      <td>...</td>\n",
       "      <td>-191</td>\n",
       "      <td>-88</td>\n",
       "      <td>290</td>\n",
       "      <td>14</td>\n",
       "      <td>194</td>\n",
       "      <td>56</td>\n",
       "      <td>303</td>\n",
       "      <td>-242</td>\n",
       "      <td>214</td>\n",
       "      <td>-28</td>\n",
       "      <td>143</td>\n",
       "      <td>231</td>\n",
       "      <td>-336</td>\n",
       "      <td>-2380</td>\n",
       "      <td>624</td>\n",
       "      <td>169</td>\n",
       "      <td>837</td>\n",
       "      <td>-36</td>\n",
       "      <td>442</td>\n",
       "      <td>-17</td>\n",
       "      <td>782</td>\n",
       "      <td>295</td>\n",
       "      <td>11</td>\n",
       "      <td>76</td>\n",
       "      <td>-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88</td>\n",
       "      <td>-27</td>\n",
       "      <td>358</td>\n",
       "      <td>42</td>\n",
       "      <td>142</td>\n",
       "      <td>359</td>\n",
       "      <td>237</td>\n",
       "      <td>1997</td>\n",
       "      <td>91</td>\n",
       "      <td>2514</td>\n",
       "      <td>-1715</td>\n",
       "      <td>-603</td>\n",
       "      <td>362</td>\n",
       "      <td>-31</td>\n",
       "      <td>-1385</td>\n",
       "      <td>-374</td>\n",
       "      <td>-213</td>\n",
       "      <td>270</td>\n",
       "      <td>603</td>\n",
       "      <td>1924</td>\n",
       "      <td>94</td>\n",
       "      <td>301</td>\n",
       "      <td>1281</td>\n",
       "      <td>128</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>-258</td>\n",
       "      <td>9</td>\n",
       "      <td>220</td>\n",
       "      <td>-58</td>\n",
       "      <td>294</td>\n",
       "      <td>95</td>\n",
       "      <td>143</td>\n",
       "      <td>-25</td>\n",
       "      <td>464</td>\n",
       "      <td>513</td>\n",
       "      <td>238</td>\n",
       "      <td>720</td>\n",
       "      <td>-204</td>\n",
       "      <td>-1772</td>\n",
       "      <td>753</td>\n",
       "      <td>315</td>\n",
       "      <td>1199</td>\n",
       "      <td>33</td>\n",
       "      <td>168</td>\n",
       "      <td>52</td>\n",
       "      <td>1138</td>\n",
       "      <td>777</td>\n",
       "      <td>41</td>\n",
       "      <td>228</td>\n",
       "      <td>-41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>484</td>\n",
       "      <td>61</td>\n",
       "      <td>118</td>\n",
       "      <td>39</td>\n",
       "      <td>-11</td>\n",
       "      <td>274</td>\n",
       "      <td>245</td>\n",
       "      <td>2128</td>\n",
       "      <td>-82</td>\n",
       "      <td>1489</td>\n",
       "      <td>-969</td>\n",
       "      <td>-909</td>\n",
       "      <td>266</td>\n",
       "      <td>-181</td>\n",
       "      <td>-900</td>\n",
       "      <td>-237</td>\n",
       "      <td>-156</td>\n",
       "      <td>115</td>\n",
       "      <td>255</td>\n",
       "      <td>1078</td>\n",
       "      <td>-24</td>\n",
       "      <td>238</td>\n",
       "      <td>1316</td>\n",
       "      <td>112</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>-357</td>\n",
       "      <td>45</td>\n",
       "      <td>430</td>\n",
       "      <td>-35</td>\n",
       "      <td>128</td>\n",
       "      <td>42</td>\n",
       "      <td>22</td>\n",
       "      <td>-131</td>\n",
       "      <td>342</td>\n",
       "      <td>142</td>\n",
       "      <td>277</td>\n",
       "      <td>307</td>\n",
       "      <td>-320</td>\n",
       "      <td>-2022</td>\n",
       "      <td>743</td>\n",
       "      <td>240</td>\n",
       "      <td>835</td>\n",
       "      <td>218</td>\n",
       "      <td>174</td>\n",
       "      <td>-110</td>\n",
       "      <td>627</td>\n",
       "      <td>170</td>\n",
       "      <td>-50</td>\n",
       "      <td>126</td>\n",
       "      <td>-91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>118</td>\n",
       "      <td>16</td>\n",
       "      <td>197</td>\n",
       "      <td>39</td>\n",
       "      <td>237</td>\n",
       "      <td>311</td>\n",
       "      <td>186</td>\n",
       "      <td>1608</td>\n",
       "      <td>204</td>\n",
       "      <td>322</td>\n",
       "      <td>-444</td>\n",
       "      <td>-254</td>\n",
       "      <td>554</td>\n",
       "      <td>16</td>\n",
       "      <td>-58</td>\n",
       "      <td>-78</td>\n",
       "      <td>-95</td>\n",
       "      <td>45</td>\n",
       "      <td>569</td>\n",
       "      <td>501</td>\n",
       "      <td>-15</td>\n",
       "      <td>181</td>\n",
       "      <td>296</td>\n",
       "      <td>-39</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-78</td>\n",
       "      <td>29</td>\n",
       "      <td>159</td>\n",
       "      <td>18</td>\n",
       "      <td>71</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>-33</td>\n",
       "      <td>159</td>\n",
       "      <td>71</td>\n",
       "      <td>134</td>\n",
       "      <td>178</td>\n",
       "      <td>-182</td>\n",
       "      <td>-179</td>\n",
       "      <td>626</td>\n",
       "      <td>156</td>\n",
       "      <td>649</td>\n",
       "      <td>57</td>\n",
       "      <td>504</td>\n",
       "      <td>-26</td>\n",
       "      <td>250</td>\n",
       "      <td>314</td>\n",
       "      <td>14</td>\n",
       "      <td>56</td>\n",
       "      <td>-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7070 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Gene Accession Number  A28102_at  AB000114_at  AB000115_at  AB000220_at  \\\n",
       "1                            151           72          281           36   \n",
       "2                            263           21          250           43   \n",
       "3                             88          -27          358           42   \n",
       "4                            484           61          118           39   \n",
       "5                            118           16          197           39   \n",
       "\n",
       "Gene Accession Number  AB000409_at  AB000449_at  AB000450_at  AB000460_at  \\\n",
       "1                             -299           57          186         1647   \n",
       "2                             -103          169          219         2043   \n",
       "3                              142          359          237         1997   \n",
       "4                              -11          274          245         2128   \n",
       "5                              237          311          186         1608   \n",
       "\n",
       "Gene Accession Number  AB000462_at  AB000464_at  AB000466_at  AB000467_at  \\\n",
       "1                              137          803         -894         -632   \n",
       "2                              188          756         -812         -700   \n",
       "3                               91         2514        -1715         -603   \n",
       "4                              -82         1489         -969         -909   \n",
       "5                              204          322         -444         -254   \n",
       "\n",
       "Gene Accession Number  AB000468_at  AB000584_at  AB000895_at  AB000896_at  \\\n",
       "1                              378          -26         -691            2   \n",
       "2                              249         -242         -369          -14   \n",
       "3                              362          -31        -1385         -374   \n",
       "4                              266         -181         -900         -237   \n",
       "5                              554           16          -58          -78   \n",
       "\n",
       "Gene Accession Number  AB000897_at  AB000905_at  AB001106_at  AB001325_at  \\\n",
       "1                             -156          155          355         1149   \n",
       "2                              -98          131          431          941   \n",
       "3                             -213          270          603         1924   \n",
       "4                             -156          115          255         1078   \n",
       "5                              -95           45          569          501   \n",
       "\n",
       "Gene Accession Number  AB002314_at  AB002315_at  AB002318_at  AB002365_at  \\\n",
       "1                             -131          158         1084           87   \n",
       "2                              -95          328         1215           53   \n",
       "3                               94          301         1281          128   \n",
       "4                              -24          238         1316          112   \n",
       "5                              -15          181          296          -39   \n",
       "\n",
       "Gene Accession Number  AB002366_at  ...  Z34822_f_at  U87593_f_at  \\\n",
       "1                              125  ...         -325          -67   \n",
       "2                              -81  ...         -191          -88   \n",
       "3                               70  ...         -258            9   \n",
       "4                               41  ...         -357           45   \n",
       "5                               -1  ...          -78           29   \n",
       "\n",
       "Gene Accession Number  U88902_cds1_f_at  AC002076_cds2_at  D64015_at  \\\n",
       "1                                   346               -68        229   \n",
       "2                                   290                14        194   \n",
       "3                                   220               -58        294   \n",
       "4                                   430               -35        128   \n",
       "5                                   159                18         71   \n",
       "\n",
       "Gene Accession Number  HG2510-HT2606_at  L10717_at  L34355_at  L78833_cds4_at  \\\n",
       "1                                   -14        108         28             349   \n",
       "2                                    56        303       -242             214   \n",
       "3                                    95        143        -25             464   \n",
       "4                                    42         22       -131             342   \n",
       "5                                    42         44        -33             159   \n",
       "\n",
       "Gene Accession Number  M13981_at  M21064_at  M93143_at  S78825_at  U11863_at  \\\n",
       "1                             61        273        384       -306      -1827   \n",
       "2                            -28        143        231       -336      -2380   \n",
       "3                            513        238        720       -204      -1772   \n",
       "4                            142        277        307       -320      -2022   \n",
       "5                             71        134        178       -182       -179   \n",
       "\n",
       "Gene Accession Number  U29175_at  U48730_at  U58516_at  U73738_at  X06956_at  \\\n",
       "1                           1582        185        511       -125        389   \n",
       "2                            624        169        837        -36        442   \n",
       "3                            753        315       1199         33        168   \n",
       "4                            743        240        835        218        174   \n",
       "5                            626        156        649         57        504   \n",
       "\n",
       "Gene Accession Number  X16699_at  X83863_at  Z17240_at  L49218_f_at  \\\n",
       "1                            -37        793        329           36   \n",
       "2                            -17        782        295           11   \n",
       "3                             52       1138        777           41   \n",
       "4                           -110        627        170          -50   \n",
       "5                            -26        250        314           14   \n",
       "\n",
       "Gene Accession Number  M71243_f_at  Z78285_f_at  \n",
       "1                              191          -37  \n",
       "2                               76          -14  \n",
       "3                              228          -41  \n",
       "4                              126          -91  \n",
       "5                               56          -25  \n",
       "\n",
       "[5 rows x 7070 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the independent dataset into a pandas DataFrame from the 'data_set_ALL_AML_independent.csv' file\n",
    "data_indepen = pd.read_csv('data_set_ALL_AML_independent.csv', index_col=1)\n",
    "\n",
    "# Clean up independent dataset as done with the training dataset\n",
    "drop_list_2 = ['call'] + ['call.' + str(x) for x in range(1,34)]\n",
    "data_indepen = data_indepen.drop(columns=drop_list_2)\n",
    "data_indepen = data_indepen.drop(labels=controls, axis=0)\n",
    "data_indepen = data_indepen.drop(['Gene Description'], axis=1)\n",
    "data_indepen.columns = data_indepen.columns.map(int)\n",
    "data_indepen.sort_index(axis=1, inplace=True)\n",
    "\n",
    "# Transpose the independent dataset so that rows are the different genes and columns are the patient samples\n",
    "gene_data_indepen = data_indepen.transpose()\n",
    "\n",
    "# Concatenate the gene expression data in the training and independent datasets into one dataframe\n",
    "gene_data = pd.concat([gene_data_train, gene_data_indepen], axis=0)\n",
    "gene_data = gene_data.astype(float)\n",
    "\n",
    "gene_data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through earlier exploratory and statistical data analysis, I identified a subset of 50 genes to use as starting features for training machine learning models and created a file containing a variety of statisical information, including an estimate of correlation between each gene and the cancer type, for each of the 50 genes. I created a summary file containing summary statistics for each of the 7129 genes during previous analysis sorted by the calculated correlation value. I have imported this file and extracted the top 25 genes with the highest correlation for each cancer type; the first 25 rows have the largest negative correlation values and correspond to the genes most correlated with AML cancer, while the last 25 rows have the largest positive correlation values and correspond to the genes most correlated with ALL cancer. Using the new dataframe containing summary statistics for the top 50 genes, I created a list containing just the genes names. Finally, I then used this list to slice out the expression data for these genes from the dataframe containing all of the gene expression data to use as my feature data (X) for subsequent machine learning.\n",
    "\n",
    "I have also merged the gene expression dataframe with the file containing the labels for cancer type (ALL or AML) to create the labeled data, and then extracted the 'cancer' column to use as the labels (y) for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene</th>\n",
       "      <th>ALL_mean</th>\n",
       "      <th>AML_mean</th>\n",
       "      <th>ALL_stdev</th>\n",
       "      <th>AML_stdev</th>\n",
       "      <th>mean_diff</th>\n",
       "      <th>std_sum</th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M55150_at</td>\n",
       "      <td>810.296296</td>\n",
       "      <td>1836.272727</td>\n",
       "      <td>338.178471</td>\n",
       "      <td>360.886434</td>\n",
       "      <td>-1025.976431</td>\n",
       "      <td>699.064905</td>\n",
       "      <td>-1.467641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U50136_rna1_at</td>\n",
       "      <td>977.777778</td>\n",
       "      <td>2562.181818</td>\n",
       "      <td>324.689003</td>\n",
       "      <td>789.747785</td>\n",
       "      <td>-1584.404040</td>\n",
       "      <td>1114.436788</td>\n",
       "      <td>-1.421708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X95735_at</td>\n",
       "      <td>349.888889</td>\n",
       "      <td>3023.636364</td>\n",
       "      <td>395.515567</td>\n",
       "      <td>1506.464687</td>\n",
       "      <td>-2673.747475</td>\n",
       "      <td>1901.980255</td>\n",
       "      <td>-1.405770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M16038_at</td>\n",
       "      <td>375.370370</td>\n",
       "      <td>1811.636364</td>\n",
       "      <td>240.297436</td>\n",
       "      <td>953.688238</td>\n",
       "      <td>-1436.265993</td>\n",
       "      <td>1193.985673</td>\n",
       "      <td>-1.202917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M23197_at</td>\n",
       "      <td>175.037037</td>\n",
       "      <td>767.272727</td>\n",
       "      <td>84.092660</td>\n",
       "      <td>411.098307</td>\n",
       "      <td>-592.235690</td>\n",
       "      <td>495.190967</td>\n",
       "      <td>-1.195974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Gene    ALL_mean     AML_mean   ALL_stdev    AML_stdev  \\\n",
       "0       M55150_at  810.296296  1836.272727  338.178471   360.886434   \n",
       "1  U50136_rna1_at  977.777778  2562.181818  324.689003   789.747785   \n",
       "2       X95735_at  349.888889  3023.636364  395.515567  1506.464687   \n",
       "3       M16038_at  375.370370  1811.636364  240.297436   953.688238   \n",
       "4       M23197_at  175.037037   767.272727   84.092660   411.098307   \n",
       "\n",
       "     mean_diff      std_sum  correlation  \n",
       "0 -1025.976431   699.064905    -1.467641  \n",
       "1 -1584.404040  1114.436788    -1.421708  \n",
       "2 -2673.747475  1901.980255    -1.405770  \n",
       "3 -1436.265993  1193.985673    -1.202917  \n",
       "4  -592.235690   495.190967    -1.195974  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in 'gene_summary.csv' file created earlier, which contains gene correlation values\n",
    "gene_summary = pd.read_csv('gene_summary.csv')\n",
    "gene_summary.rename({\"Unnamed: 0\": \"Gene\"}, axis=1, inplace=True)\n",
    "\n",
    "# Slice out top 25 genes with highest (+) correlation values and highest (-) correlation values\n",
    "top_50_gene_summary = pd.concat([gene_summary.iloc[0:25, :], gene_summary.iloc[-25:, :]], axis=0).reset_index(drop=True)\n",
    "# Create a list with the names of the top 50 genes\n",
    "top_50_gene_names = list(top_50_gene_summary['Gene'])\n",
    "\n",
    "gene_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation on training set yielded an accuracy of 97.37%\n",
      "In the training set, 37 out of 38 samples were predicted correctly.\n",
      "\n",
      "   cancer label  pred_score\n",
      "1     ALL   ALL    0.995985\n",
      "2     ALL   ALL    0.415949\n",
      "3     ALL   ALL    0.943109\n",
      "4     ALL   ALL    0.972707\n",
      "5     ALL   ALL    0.847442\n",
      "6     ALL   ALL    0.867018\n",
      "7     ALL   ALL    0.918686\n",
      "8     ALL   ALL    0.933893\n",
      "9     ALL   ALL    0.952779\n",
      "10    ALL   ALL    0.809898\n",
      "11    ALL   ALL    0.894277\n",
      "12    ALL   ALL    0.239795\n",
      "13    ALL   ALL    0.994948\n",
      "14    ALL   ALL    0.750246\n",
      "15    ALL   ALL    1.000000\n",
      "16    ALL   ALL    0.999776\n",
      "17    ALL   ALL    0.839328\n",
      "18    ALL   ALL    0.693327\n",
      "19    ALL   ALL    0.965997\n",
      "20    ALL   ALL    0.961244\n",
      "21    ALL   ALL    0.912020\n",
      "22    ALL   ALL    0.778098\n",
      "23    ALL   ALL    0.851547\n",
      "24    ALL   ALL    0.991970\n",
      "25    ALL   ALL    0.493350\n",
      "26    ALL   ALL    0.978161\n",
      "27    ALL   ALL    0.896404\n",
      "28    AML   AML    0.639611\n",
      "29    AML   AML    0.730326\n",
      "30    AML   AML    0.974271\n",
      "31    AML   AML    0.492748\n",
      "32    AML   AML    0.781426\n",
      "33    AML   AML    0.949153\n",
      "34    AML   AML    0.823126\n",
      "35    AML   ALL    0.241050\n",
      "36    AML   AML    0.999713\n",
      "37    AML   AML    0.998554\n",
      "38    AML   AML    0.664897\n",
      "\n",
      "Evaluation using the test data set yielded an accuracy of 97.06%\n",
      "In the test set, 33 out of 34 samples were predicted correctly.\n",
      "\n",
      "   cancer label  pred_score\n",
      "39    ALL   ALL    0.911789\n",
      "40    ALL   ALL    0.736970\n",
      "41    ALL   ALL    0.997648\n",
      "42    ALL   ALL    0.679333\n",
      "43    ALL   ALL    0.783054\n",
      "44    ALL   ALL    0.988635\n",
      "45    ALL   ALL    0.855300\n",
      "46    ALL   ALL    0.900434\n",
      "47    ALL   ALL    0.909357\n",
      "48    ALL   ALL    0.997266\n",
      "49    ALL   ALL    0.964823\n",
      "50    AML   AML    0.975224\n",
      "51    AML   AML    0.910370\n",
      "52    AML   AML    0.616587\n",
      "53    AML   AML    0.977843\n",
      "54    AML   AML    0.192738\n",
      "55    ALL   ALL    0.790357\n",
      "56    ALL   ALL    0.870219\n",
      "57    AML   AML    0.465305\n",
      "58    AML   AML    0.473484\n",
      "59    ALL   ALL    0.922020\n",
      "60    AML   AML    0.225258\n",
      "61    AML   AML    0.343971\n",
      "62    AML   AML    0.680028\n",
      "63    AML   AML    0.864967\n",
      "64    AML   AML    0.258448\n",
      "65    AML   AML    0.724892\n",
      "66    AML   ALL    0.623146\n",
      "67    ALL   ALL    0.052919\n",
      "68    ALL   ALL    0.918484\n",
      "69    ALL   ALL    0.976446\n",
      "70    ALL   ALL    0.873659\n",
      "71    ALL   ALL    0.561070\n",
      "72    ALL   ALL    0.860813\n"
     ]
    }
   ],
   "source": [
    "# Slice out expression data for top 50 genes from training dataset\n",
    "X_train = gene_data_train.loc[:, top_50_gene_names]\n",
    "\n",
    "# Merge with key dataframe to label data\n",
    "labeled_data_train = pd.concat([key, gene_data_train], axis=1, join='inner')\n",
    "y_train = labeled_data_train.cancer\n",
    "\n",
    "def corr_value(data1, data2):\n",
    "    \"\"\"Calculate correlation value\"\"\"\n",
    "    \n",
    "    corr_value = (np.mean(data1) - np.mean(data2)) / (np.std(data1, ddof=1) + np.std(data2, ddof=1))\n",
    "    return corr_value\n",
    "\n",
    "def fit_correlation_classifier(X, y):\n",
    "    \"\"\"Train classifier using labeled data and return dictionary containing the correlation and mean for each gene\"\"\"\n",
    "    # Extract unique classes from y and split samples into classes\n",
    "    classes = np.unique(y)\n",
    "    class_1, class_2 = classes[0], classes[1]\n",
    "    class_1_samples = [index for index, value in y.iteritems() if value == class_1]\n",
    "    class_2_samples = [index for index, value in y.iteritems() if value == class_2]\n",
    "    \n",
    "    # Initiate dictionary of weights\n",
    "    weights = {}\n",
    "    \n",
    "    # Iterate through each gene in X and calculate mean and correlation value\n",
    "    for gene, data in X.iteritems():\n",
    "        mean = (np.mean(data[class_1_samples]) + np.mean(data[class_2_samples])) / 2\n",
    "        correlation = corr_value(data[class_1_samples], data[class_2_samples])\n",
    "        weights[gene] = (correlation, mean)\n",
    "\n",
    "    # Return the dictionary\n",
    "    return weights\n",
    "\n",
    "def predict_correlation_classifier(X, weights):\n",
    "    \"Determine label by calculating weighted vote total, then return list of labels and prediction scores for each sample\"\n",
    "    labels, pred_scores = [], []\n",
    "    for sample, data in X.iterrows():\n",
    "        votes_all = []\n",
    "        votes_aml = []\n",
    "        for gene, value in data.iteritems():\n",
    "            w, b = weights[gene]\n",
    "            vote = w * (value - b)\n",
    "            if vote >= 0:\n",
    "                votes_all.append(vote)\n",
    "            else:\n",
    "                votes_aml.append(vote)\n",
    "        final_vote = sum(votes_all) + sum(votes_aml)\n",
    "        V_all = sum(votes_all)\n",
    "        V_aml = abs(sum(votes_aml))\n",
    "        if final_vote >= 0:\n",
    "            label = 'ALL'\n",
    "            pred_score = (V_all - V_aml) / (V_all + V_aml)\n",
    "        else:\n",
    "            label = 'AML'\n",
    "            pred_score = (V_aml - V_all) / (V_all + V_aml)\n",
    "        labels.append(label)\n",
    "        pred_scores.append(pred_score)\n",
    "    y_pred = pd.DataFrame(data={'label': labels, 'pred_score': pred_scores}, index=X.index)\n",
    "    return y_pred\n",
    "\n",
    "def score_corr_classifier(y_pred, y_test):\n",
    "    \"Calculate accuracy of classifier predictions\"\n",
    "    right, wrong = 0, 0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test.iloc[i] == y_pred.iloc[i, 0]:\n",
    "            right += 1\n",
    "        else: wrong += 1\n",
    "    score = (right / len(y_pred))\n",
    "    return score, right, wrong\n",
    "    \n",
    "# Train the classifier and save correlation and mean data as a dictionary\n",
    "weights = fit_correlation_classifier(X_train, y_train)\n",
    "\n",
    "# Predict labels on training set to validate classifier\n",
    "labels_train = predict_correlation_classifier(X_train, weights)\n",
    "score_train, correct_train, incorrect_train = score_corr_classifier(labels_train, y_train)\n",
    "print(\"Validation on training set yielded an accuracy of {:.2%}\".format(score_train))\n",
    "print('In the training set,', correct_train, 'out of', (correct_train + incorrect_train), 'samples were predicted correctly.\\n')\n",
    "print(pd.concat([y_train, labels_train], axis=1))\n",
    "\n",
    "# Slice out expression data for top 50 genes from independent dataset\n",
    "X_test = gene_data_indepen.loc[:, top_50_gene_names]\n",
    "\n",
    "# Generate labels for independent dataset\n",
    "labeled_data_indepen = pd.concat([key, gene_data_indepen], axis=1, join='inner')\n",
    "y_test = labeled_data_indepen.cancer\n",
    "\n",
    "# Predict labels for independent dataset\n",
    "y_pred = predict_correlation_classifier(X_test, weights)\n",
    "#print(y_pred)\n",
    "\n",
    "# Calculate accuracy score for independent dataset\n",
    "score_test, correct_test, incorrect_test = score_corr_classifier(y_pred, y_test)\n",
    "print(\"\\nEvaluation using the test data set yielded an accuracy of {:.2%}\".format(score_test))\n",
    "print('In the test set,', correct_test, 'out of', (correct_test + incorrect_test), 'samples were predicted correctly.\\n')\n",
    "print(pd.concat([y_test, y_pred], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation of the test data set using 20 predictor genes yielded an accuracy of 97.06%\n",
      "In the test set, 33 out of 34 samples were predicted correctly.\n",
      "\n",
      "   cancer label  pred_score\n",
      "39    ALL   ALL    0.935038\n",
      "40    ALL   ALL    0.446042\n",
      "41    ALL   ALL    1.000000\n",
      "42    ALL   ALL    0.928479\n",
      "43    ALL   ALL    0.942672\n",
      "44    ALL   ALL    0.997298\n",
      "45    ALL   ALL    0.959179\n",
      "46    ALL   ALL    0.923441\n",
      "47    ALL   ALL    0.787119\n",
      "48    ALL   ALL    1.000000\n",
      "49    ALL   ALL    0.969310\n",
      "50    AML   AML    0.991367\n",
      "51    AML   AML    0.892282\n",
      "52    AML   AML    0.815905\n",
      "53    AML   AML    0.972037\n",
      "54    AML   AML    0.766061\n",
      "55    ALL   ALL    0.719832\n",
      "56    ALL   ALL    0.703501\n",
      "57    AML   AML    0.631660\n",
      "58    AML   AML    0.938384\n",
      "59    ALL   ALL    0.988673\n",
      "60    AML   AML    0.242360\n",
      "61    AML   AML    0.260327\n",
      "62    AML   AML    0.940604\n",
      "63    AML   AML    0.978708\n",
      "64    AML   AML    0.831719\n",
      "65    AML   AML    0.836746\n",
      "66    AML   ALL    0.218466\n",
      "67    ALL   ALL    0.095442\n",
      "68    ALL   ALL    0.980440\n",
      "69    ALL   ALL    0.998113\n",
      "70    ALL   ALL    0.841303\n",
      "71    ALL   ALL    0.437126\n",
      "72    ALL   ALL    0.829762\n"
     ]
    }
   ],
   "source": [
    "# Slice out top 20 genes with highest (+) correlation values and highest (-) correlation values\n",
    "top_20_gene_summary = pd.concat([gene_summary.iloc[0:10, :], gene_summary.iloc[-10:, :]], axis=0).reset_index(drop=True)\n",
    "# Create a list with the names of the top 20 genes\n",
    "top_20_gene_names = list(top_20_gene_summary['Gene'])\n",
    "# Slice out expression data for top 20 genes from training dataset\n",
    "X_train_20 = gene_data_train.loc[:, top_20_gene_names]\n",
    "\n",
    "# Train the classifier and save correlation and mean data as a dictionary\n",
    "weights_20 = fit_correlation_classifier(X_train_20, y_train)\n",
    "\n",
    "# Slice out expression data for top 20 genes from independent dataset\n",
    "X_test_20 = gene_data_indepen.loc[:, top_20_gene_names]\n",
    "\n",
    "# Predict labels for independent dataset\n",
    "y_pred_20 = predict_correlation_classifier(X_test_20, weights_20)\n",
    "\n",
    "# Calculate accuracy score for independent dataset\n",
    "score_test, correct_test, incorrect_test = score_corr_classifier(y_pred_20, y_test)\n",
    "print(\"\\nEvaluation of the test data set using 20 predictor genes yielded an accuracy of {:.2%}\".format(score_test))\n",
    "print('In the test set,', correct_test, 'out of', (correct_test + incorrect_test), 'samples were predicted correctly.\\n')\n",
    "print(pd.concat([y_test, y_pred_20], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of the test data set using 10 predictor genes yielded an accuracy of 97.06%\n",
      "In the test set, 33 out of 34 samples were predicted correctly.\n",
      "\n",
      "   cancer label  pred_score\n",
      "39    ALL   ALL    0.908353\n",
      "40    ALL   ALL    0.083871\n",
      "41    ALL   ALL    1.000000\n",
      "42    ALL   ALL    0.885720\n",
      "43    ALL   ALL    0.975155\n",
      "44    ALL   ALL    1.000000\n",
      "45    ALL   ALL    0.966756\n",
      "46    ALL   ALL    0.940627\n",
      "47    ALL   ALL    0.716959\n",
      "48    ALL   ALL    1.000000\n",
      "49    ALL   ALL    0.954915\n",
      "50    AML   AML    1.000000\n",
      "51    AML   AML    1.000000\n",
      "52    AML   AML    0.592206\n",
      "53    AML   AML    0.938536\n",
      "54    AML   AML    0.694570\n",
      "55    ALL   ALL    0.494565\n",
      "56    ALL   ALL    0.524087\n",
      "57    AML   AML    0.519695\n",
      "58    AML   AML    0.985557\n",
      "59    ALL   ALL    0.997977\n",
      "60    AML   AML    0.270747\n",
      "61    AML   AML    0.687127\n",
      "62    AML   AML    0.868999\n",
      "63    AML   AML    1.000000\n",
      "64    AML   AML    0.861374\n",
      "65    AML   AML    0.762648\n",
      "66    AML   AML    0.061082\n",
      "67    ALL   AML    0.429596\n",
      "68    ALL   ALL    0.972667\n",
      "69    ALL   ALL    0.997369\n",
      "70    ALL   ALL    0.775717\n",
      "71    ALL   ALL    0.153043\n",
      "72    ALL   ALL    0.730780\n"
     ]
    }
   ],
   "source": [
    "# Slice out top 10 genes with highest (+) correlation values and highest (-) correlation values\n",
    "top_10_gene_summary = pd.concat([gene_summary.iloc[0:5, :], gene_summary.iloc[-5:, :]], axis=0).reset_index(drop=True)\n",
    "# Create a list with the names of the top 10 genes\n",
    "top_10_gene_names = list(top_10_gene_summary['Gene'])\n",
    "# Slice out expression data for top 10 genes from training dataset\n",
    "X_train_10 = gene_data_train.loc[:, top_10_gene_names]\n",
    "\n",
    "# Train the classifier and save correlation and mean data as a dictionary\n",
    "weights_10 = fit_correlation_classifier(X_train_10, y_train)\n",
    "\n",
    "# Slice out expression data for top 10 genes from independent dataset\n",
    "X_test_10 = gene_data_indepen.loc[:, top_10_gene_names]\n",
    "\n",
    "# Predict labels for independent dataset\n",
    "y_pred_10 = predict_correlation_classifier(X_test_10, weights_10)\n",
    "\n",
    "# Calculate accuracy score for independent dataset\n",
    "score_test, correct_test, incorrect_test = score_corr_classifier(y_pred_10, y_test)\n",
    "print(\"Evaluation of the test data set using 10 predictor genes yielded an accuracy of {:.2%}\".format(score_test))\n",
    "print('In the test set,', correct_test, 'out of', (correct_test + incorrect_test), 'samples were predicted correctly.\\n')\n",
    "print(pd.concat([y_test, y_pred_10], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Gene Accession Number</th>\n",
       "      <th>M55150_at</th>\n",
       "      <th>U50136_rna1_at</th>\n",
       "      <th>X95735_at</th>\n",
       "      <th>M16038_at</th>\n",
       "      <th>M23197_at</th>\n",
       "      <th>M84526_at</th>\n",
       "      <th>Y12670_at</th>\n",
       "      <th>U82759_at</th>\n",
       "      <th>D49950_at</th>\n",
       "      <th>M27891_at</th>\n",
       "      <th>X17042_at</th>\n",
       "      <th>U12471_cds1_at</th>\n",
       "      <th>U46751_at</th>\n",
       "      <th>Y00787_s_at</th>\n",
       "      <th>L08246_at</th>\n",
       "      <th>M80254_at</th>\n",
       "      <th>M62762_at</th>\n",
       "      <th>M81933_at</th>\n",
       "      <th>M96326_rna1_at</th>\n",
       "      <th>M28130_rna1_s_at</th>\n",
       "      <th>M63138_at</th>\n",
       "      <th>M11147_at</th>\n",
       "      <th>M57710_at</th>\n",
       "      <th>M81695_s_at</th>\n",
       "      <th>X85116_rna1_s_at</th>\n",
       "      <th>J05243_at</th>\n",
       "      <th>Z69881_at</th>\n",
       "      <th>U20998_at</th>\n",
       "      <th>X63469_at</th>\n",
       "      <th>D38073_at</th>\n",
       "      <th>U29175_at</th>\n",
       "      <th>M91432_at</th>\n",
       "      <th>S50223_at</th>\n",
       "      <th>AF009426_at</th>\n",
       "      <th>X15949_at</th>\n",
       "      <th>X52142_at</th>\n",
       "      <th>Z15115_at</th>\n",
       "      <th>M28170_at</th>\n",
       "      <th>L47738_at</th>\n",
       "      <th>U32944_at</th>\n",
       "      <th>M31523_at</th>\n",
       "      <th>D26156_s_at</th>\n",
       "      <th>U09087_s_at</th>\n",
       "      <th>M31211_s_at</th>\n",
       "      <th>L13278_at</th>\n",
       "      <th>X74262_at</th>\n",
       "      <th>M92287_at</th>\n",
       "      <th>U05259_rna1_at</th>\n",
       "      <th>X59417_at</th>\n",
       "      <th>U22376_cds2_s_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>654.0</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>-283.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>-88.0</td>\n",
       "      <td>835.0</td>\n",
       "      <td>-240.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>5418.0</td>\n",
       "      <td>-115.0</td>\n",
       "      <td>695.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>1766.0</td>\n",
       "      <td>1753.0</td>\n",
       "      <td>460.0</td>\n",
       "      <td>994.0</td>\n",
       "      <td>1582.0</td>\n",
       "      <td>767.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>8444.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>3349.0</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>1595.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>601.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>4778.0</td>\n",
       "      <td>9326.0</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>3105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1283.0</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1358.0</td>\n",
       "      <td>3460.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>1379.0</td>\n",
       "      <td>5345.0</td>\n",
       "      <td>2972.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>3072.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>2062.0</td>\n",
       "      <td>2325.0</td>\n",
       "      <td>17348.0</td>\n",
       "      <td>2171.0</td>\n",
       "      <td>812.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>927.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1602.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>539.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>814.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>-114.0</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>-39.0</td>\n",
       "      <td>2893.0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>898.0</td>\n",
       "      <td>822.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>435.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1184.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>895.0</td>\n",
       "      <td>3424.0</td>\n",
       "      <td>1118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1286.0</td>\n",
       "      <td>1398.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>-395.0</td>\n",
       "      <td>574.0</td>\n",
       "      <td>667.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>1496.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>609.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1268.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>2789.0</td>\n",
       "      <td>6156.0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>810.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>1697.0</td>\n",
       "      <td>3556.0</td>\n",
       "      <td>2761.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>753.0</td>\n",
       "      <td>1547.0</td>\n",
       "      <td>804.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>3118.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2723.0</td>\n",
       "      <td>2089.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>1452.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>547.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>2221.0</td>\n",
       "      <td>4926.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>7724.0</td>\n",
       "      <td>4543.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>915.0</td>\n",
       "      <td>942.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>873.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>-367.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>-304.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1401.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>-111.0</td>\n",
       "      <td>935.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>882.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1132.0</td>\n",
       "      <td>7070.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>828.0</td>\n",
       "      <td>1486.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>743.0</td>\n",
       "      <td>831.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>4261.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>731.0</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>1644.0</td>\n",
       "      <td>654.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>5403.0</td>\n",
       "      <td>5314.0</td>\n",
       "      <td>3821.0</td>\n",
       "      <td>5467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>732.0</td>\n",
       "      <td>928.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>686.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-86.0</td>\n",
       "      <td>7972.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>2453.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>949.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>9017.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>529.0</td>\n",
       "      <td>1496.0</td>\n",
       "      <td>2298.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>950.0</td>\n",
       "      <td>626.0</td>\n",
       "      <td>1423.0</td>\n",
       "      <td>476.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>3425.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>649.0</td>\n",
       "      <td>3502.0</td>\n",
       "      <td>1322.0</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>5354.0</td>\n",
       "      <td>5216.0</td>\n",
       "      <td>3469.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Gene Accession Number  M55150_at  U50136_rna1_at  X95735_at  M16038_at  \\\n",
       "1                          654.0          1124.0      298.0      365.0   \n",
       "2                         1283.0          1062.0      307.0      624.0   \n",
       "3                         1286.0          1398.0      309.0      108.0   \n",
       "4                          915.0           942.0      693.0      873.0   \n",
       "5                          732.0           928.0      713.0      686.0   \n",
       "\n",
       "Gene Accession Number  M23197_at  M84526_at  Y12670_at  U82759_at  D49950_at  \\\n",
       "1                          261.0     -283.0      600.0      393.0       75.0   \n",
       "2                          101.0      -65.0      337.0      118.0      129.0   \n",
       "3                          309.0     -395.0      574.0      667.0       44.0   \n",
       "4                          288.0     -367.0      716.0      410.0      218.0   \n",
       "5                          395.0      290.0      524.0      119.0      110.0   \n",
       "\n",
       "Gene Accession Number  M27891_at  X17042_at  U12471_cds1_at  U46751_at  \\\n",
       "1                          303.0      177.0           160.0     1298.0   \n",
       "2                         1358.0     3460.0           134.0     1379.0   \n",
       "3                          254.0      416.0           167.0     1496.0   \n",
       "4                         -304.0      392.0           104.0     1401.0   \n",
       "5                          -86.0     7972.0           145.0     1894.0   \n",
       "\n",
       "Gene Accession Number  Y00787_s_at  L08246_at  M80254_at  M62762_at  \\\n",
       "1                            333.0      543.0      -88.0      835.0   \n",
       "2                           5345.0     2972.0      109.0     3072.0   \n",
       "3                            301.0      485.0      -53.0      609.0   \n",
       "4                            241.0      740.0     -111.0      935.0   \n",
       "5                             93.0     2453.0      139.0     1665.0   \n",
       "\n",
       "Gene Accession Number  M81933_at  M96326_rna1_at  M28130_rna1_s_at  M63138_at  \\\n",
       "1                         -240.0           782.0             292.0     1410.0   \n",
       "2                          -11.0           368.0            2062.0     2325.0   \n",
       "3                          124.0          1268.0             -25.0     2789.0   \n",
       "4                          167.0           882.0              -2.0     1132.0   \n",
       "5                          114.0           949.0             138.0     1269.0   \n",
       "\n",
       "Gene Accession Number  M11147_at  M57710_at  M81695_s_at  X85116_rna1_s_at  \\\n",
       "1                         5418.0     -115.0        695.0             162.0   \n",
       "2                        17348.0     2171.0        812.0             169.0   \n",
       "3                         6156.0      -26.0        810.0             -18.0   \n",
       "4                         7070.0       22.0        599.0             199.0   \n",
       "5                         9017.0      244.0        282.0             334.0   \n",
       "\n",
       "Gene Accession Number  J05243_at  Z69881_at  U20998_at  X63469_at  D38073_at  \\\n",
       "1                          610.0     1766.0     1753.0      460.0      994.0   \n",
       "2                          927.0     2015.0     1602.0      151.0      539.0   \n",
       "3                         1697.0     3556.0     2761.0      230.0     1441.0   \n",
       "4                          425.0      828.0     1486.0      314.0      680.0   \n",
       "5                          529.0     1496.0     2298.0      632.0      950.0   \n",
       "\n",
       "Gene Accession Number  U29175_at  M91432_at  S50223_at  AF009426_at  \\\n",
       "1                         1582.0      767.0      268.0         36.0   \n",
       "2                          624.0      814.0      346.0         58.0   \n",
       "3                          753.0     1547.0      804.0         63.0   \n",
       "4                          743.0      831.0      452.0         38.0   \n",
       "5                          626.0     1423.0      476.0        120.0   \n",
       "\n",
       "Gene Accession Number  X15949_at  X52142_at  Z15115_at  M28170_at  L47738_at  \\\n",
       "1                          277.0      107.0     8444.0      397.0      571.0   \n",
       "2                          104.0     -114.0     1920.0      -39.0     2893.0   \n",
       "3                           91.0      -13.0     3118.0      183.0     2723.0   \n",
       "4                          403.0      190.0     4261.0      363.0      731.0   \n",
       "5                          416.0      361.0     3425.0      251.0      649.0   \n",
       "\n",
       "Gene Accession Number  U32944_at  M31523_at  D26156_s_at  U09087_s_at  \\\n",
       "1                         3349.0     1320.0       1595.0        358.0   \n",
       "2                         1002.0      898.0        822.0         82.0   \n",
       "3                         2089.0      597.0       1452.0        263.0   \n",
       "4                         1625.0     1644.0        654.0        218.0   \n",
       "5                         3502.0     1322.0       1011.0        186.0   \n",
       "\n",
       "Gene Accession Number  M31211_s_at  L13278_at  X74262_at  M92287_at  \\\n",
       "1                            601.0      193.0     1372.0     4778.0   \n",
       "2                            435.0       31.0     1184.0     2700.0   \n",
       "3                            547.0      198.0     2221.0     4926.0   \n",
       "4                            472.0       91.0     1051.0     5403.0   \n",
       "5                            661.0      194.0     1370.0     3440.0   \n",
       "\n",
       "Gene Accession Number  U05259_rna1_at  X59417_at  U22376_cds2_s_at  \n",
       "1                              9326.0     3016.0            3105.0  \n",
       "2                               895.0     3424.0            1118.0  \n",
       "3                               628.0     7724.0            4543.0  \n",
       "4                              5314.0     3821.0            5467.0  \n",
       "5                              5354.0     5216.0            3469.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slice out top 25 genes with highest (+) correlation values and highest (-) correlation values\n",
    "top_50_gene_summary = pd.concat([gene_summary.iloc[0:25, :], gene_summary.iloc[-25:, :]], axis=0).reset_index(drop=True)\n",
    "# Create a list with the names of the top 50 genes\n",
    "top_50_gene_names = list(top_50_gene_summary['Gene'])\n",
    "# Slice out expression data for top 50 genes to use as X data for training classifiers\n",
    "X = gene_data.loc[:, top_50_gene_names]\n",
    "\n",
    "# Merge the key dataframe containing the cancer labels with the gene_data dataframe to assign the cancer type to each patient sample\n",
    "labeled_data = pd.concat([key, gene_data], axis=1, join='inner')\n",
    "\n",
    "# Pull out just the labels from the labelled data to use for training classifiers\n",
    "y = labeled_data.cancer\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature data and labels are now readily available as X and y and can be used to assess different machine learning algorithms. As a first step, I have decided to analyze the performance of a k-Nearest Neighbors classifier. Before jumping into hyperparameter tuning and cross-validation, however, I have first split my dataset into training and test sets. The training data, X_train and y_train, were used for hyperparameter tuning and cross-validation, while the test sets, X_test and y_test, were reserved for the final performance assesment of the optimized kNN classifier.\n",
    "As the gene expression data can vary widely from gene to gene, the data must also be preprocessed before training classifiers to standardize the variance. Therefore, I have also used transformers to standardize or normalize the gene expression data in subsequent analyses.\n",
    "\n",
    "The kNN classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbors is 2\n",
      "\n",
      "Accuracy: 88.89%\n",
      "\n",
      "Confusion matrix: \n",
      "[[12  0]\n",
      " [ 2  4]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ALL       0.86      1.00      0.92        12\n",
      "         AML       1.00      0.67      0.80         6\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.93      0.83      0.86        18\n",
      "weighted avg       0.90      0.89      0.88        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid, KDTree\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Split data into train and test sets; stratify split so that AML samples have the same proportion in both train & test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=1582, stratify=y)\n",
    "\n",
    "# Initiate normalizer, kNN Classifier, and pipeline\n",
    "knn_pipeline = Pipeline(steps=[('normalizer', Normalizer()), ('knn', KNeighborsClassifier())])\n",
    "\n",
    "# Set list of parameters to test\n",
    "parameters = {'knn__n_neighbors': np.arange(1, 20)}\n",
    "\n",
    "# Perform GridSearch on list of parameters, then fit the model\n",
    "knn_cv = GridSearchCV(knn_pipeline, param_grid=parameters, cv=5, iid=False)\n",
    "knn_cv.fit(X_train, y_train)\n",
    "y_pred = knn_cv.predict(X_test)\n",
    "\n",
    "# Print out classification metrics\n",
    "print('Best n_neighbors is {}\\n'.format(knn_cv.best_params_['knn__n_neighbors']))\n",
    "print('Accuracy: {:.2%}\\n'.format(knn_cv.score(X_test, y_test)))\n",
    "print('Confusion matrix: \\n{}\\n'.format(confusion_matrix(y_test, y_pred)))\n",
    "print('Classification report: \\n{}'.format(classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next section, I have assessed the performance of a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.44%\n",
      "\n",
      "Confusion matrix: \n",
      "[[12  0]\n",
      " [ 1  5]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ALL       0.92      1.00      0.96        12\n",
      "         AML       1.00      0.83      0.91         6\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        18\n",
      "   macro avg       0.96      0.92      0.93        18\n",
      "weighted avg       0.95      0.94      0.94        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "logreg_pipeline = Pipeline(steps=[('normalizer', Normalizer()),('logreg', LogisticRegression(solver='lbfgs'))])\n",
    "\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "logreg_y_pred = logreg_pipeline.predict(X_test)\n",
    "y_pred_prob = logreg_pipeline.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob, pos_label='AML')\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# Print classification metrics\n",
    "print('Accuracy: {:.2%}\\n'.format(logreg_pipeline.score(X_test, y_test)))\n",
    "print('Confusion matrix: \\n{}\\n'.format(confusion_matrix(y_test, logreg_y_pred)))\n",
    "print('Classification report: \\n{}'.format(classification_report(y_test, logreg_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C is 0.5\n",
      "Best gamma is 0.5\n",
      "\n",
      "Accuracy: 94.44%\n",
      "\n",
      "Confusion matrix: \n",
      "[[11  1]\n",
      " [ 0  6]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ALL       1.00      0.92      0.96        12\n",
      "         AML       0.86      1.00      0.92         6\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        18\n",
      "   macro avg       0.93      0.96      0.94        18\n",
      "weighted avg       0.95      0.94      0.95        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initiate SVC pipeline\n",
    "svc_pipeline = Pipeline(steps=[('normalizer', Normalizer()),('svc', SVC())])\n",
    "\n",
    "# Set parameters to test\n",
    "parameters = {'svc__C': [0.01, 0.1, 0.5, 0.75, 1, 1.25, 1.5, 2], 'svc__gamma': [0.01, 0.1, 0.5, 0.75, 1, 1.25, 1.5, 2, 2.5]}\n",
    "\n",
    "# Perform GridSearch on list of parameters, then fit the model\n",
    "svc_cv = GridSearchCV(svc_pipeline, param_grid=parameters, cv=5, iid=False)\n",
    "svc_cv.fit(X_train, y_train)\n",
    "svc_y_pred = svc_cv.predict(X_test)\n",
    "\n",
    "# Print out classification metrics\n",
    "print('Best C is {}'.format(svc_cv.best_params_['svc__C']))\n",
    "print('Best gamma is {}\\n'.format(svc_cv.best_params_['svc__gamma']))\n",
    "print('Accuracy: {:.2%}\\n'.format(svc_cv.score(X_test, y_test)))\n",
    "print('Confusion matrix: \\n{}\\n'.format(confusion_matrix(y_test, svc_y_pred)))\n",
    "print('Classification report: \\n{}'.format(classification_report(y_test, svc_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest:\n",
    "Note: for Random Forest Classifiers, the best value for n_estimators returned by GridSearchCV varies if random_state is not specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_estimators is: 25\n",
      "\n",
      "Accuracy: 100.00%\n",
      "\n",
      "Confusion matrix: \n",
      "[[12  0]\n",
      " [ 0  6]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ALL       1.00      1.00      1.00        12\n",
      "         AML       1.00      1.00      1.00         6\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        18\n",
      "   macro avg       1.00      1.00      1.00        18\n",
      "weighted avg       1.00      1.00      1.00        18\n",
      "\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       0.008584      0.001343         0.001782        0.000393   \n",
      "1       0.015957      0.000892         0.002394        0.000489   \n",
      "2       0.028524      0.000490         0.003391        0.000489   \n",
      "3       0.045858      0.005112         0.004903        0.000690   \n",
      "4       0.059458      0.008079         0.006185        0.000748   \n",
      "5       0.293062      0.045976         0.030699        0.006668   \n",
      "\n",
      "  param_rfc__n_estimators                      params  split0_test_score  \\\n",
      "0                      10   {'rfc__n_estimators': 10}                1.0   \n",
      "1                      25   {'rfc__n_estimators': 25}                1.0   \n",
      "2                      50   {'rfc__n_estimators': 50}                1.0   \n",
      "3                      75   {'rfc__n_estimators': 75}                1.0   \n",
      "4                     100  {'rfc__n_estimators': 100}                1.0   \n",
      "5                     500  {'rfc__n_estimators': 500}                1.0   \n",
      "\n",
      "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
      "0                1.0           0.909091           0.909091                1.0   \n",
      "1                1.0           1.000000           0.909091                1.0   \n",
      "2                1.0           0.909091           0.909091                1.0   \n",
      "3                1.0           0.909091           0.909091                1.0   \n",
      "4                1.0           0.909091           0.909091                1.0   \n",
      "5                1.0           1.000000           0.909091                1.0   \n",
      "\n",
      "   mean_test_score  std_test_score  rank_test_score  \n",
      "0         0.963636        0.044536                3  \n",
      "1         0.981818        0.036364                1  \n",
      "2         0.963636        0.044536                3  \n",
      "3         0.963636        0.044536                3  \n",
      "4         0.963636        0.044536                3  \n",
      "5         0.981818        0.036364                1  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initiate RandomForest pipeline\n",
    "rfc_pipeline = Pipeline(steps=[('normalizer', Normalizer()),('rfc', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Set parameters to test\n",
    "parameters = {'rfc__n_estimators': [10, 25, 50, 75, 100, 500]}\n",
    "\n",
    "# Perform GridSearch on list of parameters, then fit the model\n",
    "rfc_cv = GridSearchCV(rfc_pipeline, param_grid=parameters, cv=5, iid=False, return_train_score=False)\n",
    "\n",
    "# Fit and predict classifier\n",
    "rfc_cv.fit(X_train, y_train)\n",
    "rfc_y_pred = rfc_cv.predict(X_test)\n",
    "\n",
    "# Print out classification metrics\n",
    "print('Best n_estimators is: {}\\n'.format(rfc_cv.best_params_['rfc__n_estimators']))\n",
    "print('Accuracy: {:.2%}\\n'.format(rfc_cv.score(X_test, y_test)))\n",
    "print('Confusion matrix: \\n{}\\n'.format(confusion_matrix(y_test, rfc_y_pred)))\n",
    "print('Classification report: \\n{}'.format(classification_report(y_test, rfc_y_pred)))\n",
    "\n",
    "print(pd.DataFrame(rfc_cv.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the kNN classifier, I then wanted to assess the smallest subset of genes that could be used to train and fit the classifier without reducing the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.31%\n",
      "\n",
      "Confusion matrix: \n",
      "[[38  0]\n",
      " [12  8]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ALL       0.76      1.00      0.86        38\n",
      "         AML       1.00      0.40      0.57        20\n",
      "\n",
      "   micro avg       0.79      0.79      0.79        58\n",
      "   macro avg       0.88      0.70      0.72        58\n",
      "weighted avg       0.84      0.79      0.76        58\n",
      "\n",
      "Best n_neighbors is 2\n",
      "\n",
      "Accuracy: 87.93%\n",
      "\n",
      "Confusion matrix: \n",
      "[[38  0]\n",
      " [ 7 13]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ALL       0.84      1.00      0.92        38\n",
      "         AML       1.00      0.65      0.79        20\n",
      "\n",
      "   micro avg       0.88      0.88      0.88        58\n",
      "   macro avg       0.92      0.82      0.85        58\n",
      "weighted avg       0.90      0.88      0.87        58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Slice out top 10 genes with highest (+) correlation values and highest (-) correlation values\n",
    "top_10_gene_summary = pd.concat([gene_summary.iloc[0:1, :], gene_summary.iloc[-1:, :]], axis=0).reset_index(drop=True)\n",
    "# Create a list with the names of the top 10 genes\n",
    "top_10_gene_names = list(top_10_gene_summary['Gene'])\n",
    "# Slice out expression data for top 10 genes from training dataset\n",
    "X_10 = gene_data.loc[:, top_10_gene_names]\n",
    "\n",
    "# Merge the key dataframe containing the cancer labels with the gene_data dataframe to assign the cancer type to each patient sample\n",
    "labeled_data = pd.concat([key, gene_data], axis=1, join='inner')\n",
    "\n",
    "# Pull out just the labels from the labelled data to use for training classifiers\n",
    "y = labeled_data.cancer\n",
    "\n",
    "# Split data into train and test sets; stratify split so that AML samples have the same proportion in both train & test sets\n",
    "X_train_10, X_test_10, y_train_10, y_test_10 = train_test_split(X_10, y, test_size=.8, random_state=1582, stratify=y)\n",
    "\n",
    "# Initiate Normalizer, LogisticRegression, and Pipeline, then fit & predict\n",
    "logreg_pipeline = Pipeline(steps=[('normalizer', Normalizer()),('logreg', LogisticRegression(solver='lbfgs'))])\n",
    "logreg_pipeline.fit(X_train_10, y_train_10)\n",
    "logreg_y_pred = logreg_pipeline.predict(X_test_10)\n",
    "\n",
    "# Print classification metrics\n",
    "print('Accuracy: {:.2%}\\n'.format(logreg_pipeline.score(X_test_10, y_test_10)))\n",
    "print('Confusion matrix: \\n{}\\n'.format(confusion_matrix(y_test_10, logreg_y_pred)))\n",
    "print('Classification report: \\n{}'.format(classification_report(y_test_10, logreg_y_pred)))\n",
    "\n",
    "# Initiate normalizer, kNN Classifier, and pipeline\n",
    "knn_pipeline = Pipeline(steps=[('normalizer', Normalizer()), ('knn', KNeighborsClassifier())])\n",
    "\n",
    "# Set list of parameters to test\n",
    "parameters = {'knn__n_neighbors': np.arange(1, 10)}\n",
    "\n",
    "# Perform GridSearch on list of parameters, then fit the model\n",
    "knn_cv = GridSearchCV(knn_pipeline, param_grid=parameters, cv=5, iid=False)\n",
    "knn_cv.fit(X_train_10, y_train_10)\n",
    "y_pred_10 = knn_cv.predict(X_test_10)\n",
    "\n",
    "# Print out classification metrics\n",
    "print('Best n_neighbors is {}\\n'.format(knn_cv.best_params_['knn__n_neighbors']))\n",
    "print('Accuracy: {:.2%}\\n'.format(knn_cv.score(X_test_10, y_test_10)))\n",
    "print('Confusion matrix: \\n{}\\n'.format(confusion_matrix(y_test_10, y_pred_10)))\n",
    "print('Classification report: \\n{}'.format(classification_report(y_test_10, y_pred_10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all of the models I have tried so far yield the same performance, I would like to try reducing the number of features/dimensions using PCA. First, I will use PCA with the reduced set of 50 genes to determine the intrinsic dimension of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_components is 9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# First, I will investigate the intrinsic dimension of the dataset\n",
    "# Initiate PCA transformer\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Plot the PCA components vs. PCA explained varaince\n",
    "features = range(pca.n_components_)\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.bar(features, pca.explained_variance_ratio_)\n",
    "plt.xlabel('PCA components')\n",
    "plt.ylabel('PCA explained variance')\n",
    "plt.title('PCA Components vs Explained Variance Using 50 Gene Subset')\n",
    "plt.savefig(fname='images\\PCA_50_genes.png', dpi='figure', format='png', bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()\n",
    "\n",
    "# Next, I will optimize the number of components using GridSearchCV\n",
    "# Initiate Normalizer, PCA transformer, and Pipeline\n",
    "normalizer = Normalizer()\n",
    "pca = PCA()\n",
    "pca_pipeline = Pipeline(steps=[('normalizer', normalizer), ('pca', pca)])\n",
    "\n",
    "# Set parameters to test\n",
    "pca_params = {'pca__n_components': np.arange(1, 25)}\n",
    "\n",
    "# Perform GridSearchCV to optimize parameters, then fit the model\n",
    "pca_cv = GridSearchCV(pca_pipeline, param_grid=pca_params, cv=5, iid=False)\n",
    "pca_cv.fit(X_train, y_train)\n",
    "print('Best n_components is {}'.format(pca_cv.best_params_['pca__n_components']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will do the same thing using the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split data into train and test sets; stratify split so that AML samples have the same proportion in both train & test sets\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(gene_data, key['cancer'], test_size=.25, random_state=1582, stratify=y)\n",
    "\n",
    "# Initiate PCA transformer\n",
    "pca_all = PCA()\n",
    "pca_all.fit(X_train_all)\n",
    "\n",
    "features = range(pca_all.n_components_)\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.bar(features, pca_all.explained_variance_ratio_)\n",
    "plt.xlabel('PCA components')\n",
    "plt.ylabel('PCA explained variance')\n",
    "plt.title('PCA Components vs Explained Variance Using All Genes')\n",
    "plt.savefig(fname='images\\PCA_all_genes.png', dpi='figure', format='png', bbox_inches='tight', pad_inches=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA feature reduction of the 50 gene subset, followed by kNN classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbors is 2\n",
      "Best n_components is 1\n",
      "\n",
      "Accuracy: 88.89%\n",
      "\n",
      "Confusion matrix: \n",
      "[[12  0]\n",
      " [ 2  4]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ALL       0.86      1.00      0.92        12\n",
      "         AML       1.00      0.67      0.80         6\n",
      "\n",
      "   micro avg       0.89      0.89      0.89        18\n",
      "   macro avg       0.93      0.83      0.86        18\n",
      "weighted avg       0.90      0.89      0.88        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initiate pipeline for normalization, PCA reduction, and k-nearest neighbors classification\n",
    "normalizer = Normalizer()\n",
    "pca = PCA()\n",
    "knn = KNeighborsClassifier()\n",
    "pca_pipeline = Pipeline(steps=[('normalizer', normalizer), ('pca', pca), ('knn', knn)])\n",
    "\n",
    "# Set parameters to test with GridSearchCV\n",
    "pca_params = {'pca__n_components': range(1, 20), 'knn__n_neighbors': range(1, 20)}\n",
    "\n",
    "# Perform GridSearch on list of parameters, then fit the model\n",
    "pca_knn_cv = GridSearchCV(pca_pipeline, param_grid=pca_params, cv=5, iid=False)\n",
    "pca_knn_cv.fit(X_train, y_train)\n",
    "y_pred_pca = pca_knn_cv.predict(X_test)\n",
    "\n",
    "# Print out classification metrics\n",
    "print('Best n_neighbors is {}'.format(pca_knn_cv.best_params_['knn__n_neighbors']))\n",
    "print('Best n_components is {}\\n'.format(pca_knn_cv.best_params_['pca__n_components']))\n",
    "print('Accuracy: {:.2%}\\n'.format(pca_knn_cv.score(X_test, y_test)))\n",
    "print('Confusion matrix: \\n{}\\n'.format(confusion_matrix(y_test, y_pred_pca)))\n",
    "print('Classification report: \\n{}'.format(classification_report(y_test, y_pred_pca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA reduction of the whole 7070 gene dataset, followed by kNN classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbors is 1\n",
      "Best n_components is 2\n",
      "\n",
      "Accuracy: 83.33%\n",
      "\n",
      "Confusion matrix: \n",
      "[[11  1]\n",
      " [ 2  4]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ALL       0.85      0.92      0.88        12\n",
      "         AML       0.80      0.67      0.73         6\n",
      "\n",
      "   micro avg       0.83      0.83      0.83        18\n",
      "   macro avg       0.82      0.79      0.80        18\n",
      "weighted avg       0.83      0.83      0.83        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initiate pipeline for normalization, PCA reduction, and k-nearest neighbors classification\n",
    "normalizer = Normalizer()\n",
    "pca = PCA()\n",
    "knn = KNeighborsClassifier()\n",
    "pca_pipeline = Pipeline(steps=[('normalizer', normalizer), ('pca', pca), ('knn', knn)])\n",
    "\n",
    "# Set parameters to test with GridSearchCV\n",
    "pca_params = {'pca__n_components': range(1, 20), 'knn__n_neighbors': np.arange(1, 20)}\n",
    "\n",
    "# Perform GridSearch on list of parameters, then fit the model\n",
    "pca_knn_cv = GridSearchCV(pca_pipeline, param_grid=pca_params, cv=5, iid=False)\n",
    "pca_knn_cv.fit(X_train_all, y_train_all)\n",
    "y_pred_pca = pca_knn_cv.predict(X_test_all)\n",
    "\n",
    "# Print out classification metrics\n",
    "print('Best n_neighbors is {}'.format(pca_knn_cv.best_params_['knn__n_neighbors']))\n",
    "print('Best n_components is {}\\n'.format(pca_knn_cv.best_params_['pca__n_components']))\n",
    "print('Accuracy: {:.2%}\\n'.format(pca_knn_cv.score(X_test_all, y_test_all)))\n",
    "print('Confusion matrix: \\n{}\\n'.format(confusion_matrix(y_test_all, y_pred_pca)))\n",
    "print('Classification report: \\n{}'.format(classification_report(y_test_all, y_pred_pca)))\n",
    "\n",
    "#print(pd.DataFrame(pca_knn_cv.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a quick test using NMF to reduce the dimension of the 50-gene dataset. GridSearchCV is used to tune n_components and n_neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbors is 3\n",
      "Best n_components is 8\n",
      "\n",
      "Accuracy: 100.00%\n",
      "\n",
      "Confusion matrix: \n",
      "[[12  0]\n",
      " [ 0  6]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ALL       1.00      1.00      1.00        12\n",
      "         AML       1.00      1.00      1.00         6\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        18\n",
      "   macro avg       1.00      1.00      1.00        18\n",
      "weighted avg       1.00      1.00      1.00        18\n",
      "\n",
      "   M55150_at  U50136_rna1_at  X95735_at  M16038_at  M23197_at  M84526_at  \\\n",
      "0   0.975907        0.900513   0.801871   0.685811   0.685307   0.664782   \n",
      "1   0.651977        0.615504   0.461377   0.750451   0.308652   0.362456   \n",
      "2   0.320256        0.430192   0.713663   0.583053   0.867771   0.487244   \n",
      "3   0.391057        0.210718   0.349166   0.122822   0.264424   0.586123   \n",
      "4   0.129282        0.435987   0.173000   0.263339   0.285427   0.287710   \n",
      "5   0.230978        0.138700   0.096955   0.124567   0.214983   0.133375   \n",
      "6   0.000000        0.000000   0.248644   0.088438   0.276250   0.215310   \n",
      "7   0.000000        0.000000   0.100152   0.204600   0.083026   0.039224   \n",
      "\n",
      "   Y12670_at  U82759_at  D49950_at  M27891_at  X17042_at  U12471_cds1_at  \\\n",
      "0   0.712942   1.015125   0.881925   0.597003   0.717148        0.863717   \n",
      "1   0.407125   0.681111   0.574072   0.369229   0.643003        0.520600   \n",
      "2   0.264321   0.214316   0.570685   0.946897   0.441213        0.179843   \n",
      "3   0.330362   0.236511   0.320061   0.170914   1.019291        0.444718   \n",
      "4   0.331393   0.104006   0.419278   0.218699   0.000000        0.336337   \n",
      "5   0.164244   0.173642   0.000000   0.249683   0.076470        0.257936   \n",
      "6   0.265618   0.009275   0.413306   0.437259   0.291405        0.429880   \n",
      "7   0.077274   0.093109   0.000000   0.071742   0.098862        0.021803   \n",
      "\n",
      "   U46751_at  Y00787_s_at  L08246_at  M80254_at  M62762_at  M81933_at  \\\n",
      "0   0.764339     0.711778   0.778474   0.858964   0.805335   0.760618   \n",
      "1   0.634617     0.953112   0.775702   0.823917   0.539966   0.552061   \n",
      "2   0.284260     0.096116   0.288269   0.023141   0.736025   0.087622   \n",
      "3   0.273733     0.115831   0.000000   0.436965   0.158059   0.739140   \n",
      "4   0.220721     0.247088   0.145405   0.156105   0.142933   0.190816   \n",
      "5   0.104240     0.116097   0.134412   0.147525   0.323643   0.446336   \n",
      "6   0.261232     0.002436   0.632846   0.267364   0.573702   1.390924   \n",
      "7   0.062674     0.042718   0.089051   0.088288   0.108555   0.066659   \n",
      "\n",
      "   M96326_rna1_at  M28130_rna1_s_at  M63138_at  M11147_at  M57710_at  \\\n",
      "0        0.773076          0.703667   0.678485   0.742671   0.514606   \n",
      "1        0.343470          0.779725   0.385498   0.610236   0.472347   \n",
      "2        0.203088          0.025113   0.721909   0.715826   0.659931   \n",
      "3        1.149515          0.239948   0.270014   0.042119   0.215039   \n",
      "4        0.229053          0.290776   0.175878   0.066349   0.086587   \n",
      "5        0.016724          0.106071   0.286292   0.338260   0.410797   \n",
      "6        0.000000          0.050546   0.415614   0.535706   0.650385   \n",
      "7        0.000000          0.062978   0.060385   0.153968   0.135543   \n",
      "\n",
      "   M81695_s_at  X85116_rna1_s_at  J05243_at  Z69881_at  U20998_at  X63469_at  \\\n",
      "0     0.817525          0.667392   1.062122   1.067248   0.994828   1.126059   \n",
      "1     0.619737          0.567188   0.092259   0.000000   0.149048   0.006177   \n",
      "2     0.600493          0.038163   0.042146   0.140350   0.100698   0.325193   \n",
      "3     0.060724          0.915678   0.166143   0.173450   0.265949   0.000000   \n",
      "4     0.272800          0.110474   0.343395   0.288230   0.076774   0.401828   \n",
      "5     0.210801          0.138294   0.857714   0.508762   0.268290   0.000354   \n",
      "6     0.033027          0.354392   0.245945   0.463852   0.000000   0.338328   \n",
      "7     0.016258          0.192068   0.000000   0.000000   0.467575   0.502396   \n",
      "\n",
      "   D38073_at  U29175_at  M91432_at  S50223_at  AF009426_at  X15949_at  \\\n",
      "0   1.079144   0.719932   0.892674   1.222536     0.923482   1.052617   \n",
      "1   0.050848   0.183871   0.146001   0.126408     0.207308   0.096978   \n",
      "2   0.083547   0.102086   0.123297   0.000000     0.057177   0.313779   \n",
      "3   0.280268   0.293245   0.159957   0.044972     0.490781   0.318812   \n",
      "4   0.096972   0.602053   0.171657   0.000000     0.343250   0.328728   \n",
      "5   0.171286   0.316191   0.254730   0.327827     0.206779   0.063249   \n",
      "6   0.521840   0.326536   0.200770   0.197858     0.319987   0.545450   \n",
      "7   0.299362   0.389481   0.369142   0.326104     0.657551   0.510773   \n",
      "\n",
      "   X52142_at  Z15115_at  M28170_at  L47738_at  U32944_at  M31523_at  \\\n",
      "0   1.045356   0.871390   0.700114   0.934075   0.921613   0.763411   \n",
      "1   0.092741   0.112801   0.191825   0.215868   0.064308   0.151631   \n",
      "2   0.104188   0.100669   0.070688   0.090591   0.183274   0.048319   \n",
      "3   0.589725   0.175742   0.488776   0.058021   0.281950   0.261318   \n",
      "4   0.537080   0.476785   0.919977   0.188328   0.212052   0.373548   \n",
      "5   0.032341   0.040677   0.288673   0.631823   0.107178   0.166841   \n",
      "6   0.541318   0.246144   0.341097   0.000000   0.212216   0.557429   \n",
      "7   0.358826   0.637243   0.525420   0.214351   0.480248   0.540533   \n",
      "\n",
      "   D26156_s_at  U09087_s_at  M31211_s_at  L13278_at  X74262_at  M92287_at  \\\n",
      "0     0.950340     1.439238     0.946249   0.975715   0.987348   1.101000   \n",
      "1     0.202300     0.038980     0.111354   0.092837   0.098382   0.000000   \n",
      "2     0.103697     0.028290     0.076255   0.219872   0.137517   0.055798   \n",
      "3     0.140835     0.015455     0.192968   0.189024   0.212889   0.000000   \n",
      "4     0.603078     0.286111     0.170525   0.346129   0.077595   0.109826   \n",
      "5     0.293203     0.013065     0.343627   0.157430   0.296443   0.073295   \n",
      "6     0.414746     0.000000     0.542324   0.414149   0.139748   1.283892   \n",
      "7     0.018309     0.096048     0.294672   0.429199   0.335292   0.151323   \n",
      "\n",
      "   U05259_rna1_at  X59417_at  U22376_cds2_s_at  \n",
      "0        0.778906   1.033357          0.894736  \n",
      "1        0.097691   0.070335          0.082837  \n",
      "2        0.134891   0.021303          0.001154  \n",
      "3        0.136679   0.105398          0.470220  \n",
      "4        0.854021   0.150381          0.332218  \n",
      "5        0.068849   0.251038          0.089788  \n",
      "6        0.785759   0.703688          0.466256  \n",
      "7        0.245795   0.324694          0.572956  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Initiate pipeline for normalization, PCA reduction, and k-nearest neighbors classification\n",
    "scaler = MinMaxScaler(feature_range=(1,2))\n",
    "nmf = NMF()\n",
    "knn = KNeighborsClassifier()\n",
    "nmf_pipeline = Pipeline(steps=[('scaler', scaler), ('nmf', nmf), ('knn', knn)])\n",
    "\n",
    "# Set parameters to test with GridSearchCV\n",
    "nmf_params = {'nmf__n_components': [2,4,6,8,10,12,14,16,18,20], 'knn__n_neighbors': np.arange(1, 20)}\n",
    "\n",
    "# Perform GridSearch on list of parameters, then fit the model\n",
    "nmf_knn_cv = GridSearchCV(nmf_pipeline, param_grid=nmf_params, cv=5, iid=False)\n",
    "nmf_knn_cv.fit(X_train, y_train)\n",
    "y_pred_nmf = nmf_knn_cv.predict(X_test)\n",
    "\n",
    "# Print out classification metrics\n",
    "print('Best n_neighbors is {}'.format(nmf_knn_cv.best_params_['knn__n_neighbors']))\n",
    "print('Best n_components is {}\\n'.format(nmf_knn_cv.best_params_['nmf__n_components']))\n",
    "print('Accuracy: {:.2%}\\n'.format(nmf_knn_cv.score(X_test, y_test)))\n",
    "print('Confusion matrix: \\n{}\\n'.format(confusion_matrix(y_test, y_pred_nmf)))\n",
    "print('Classification report: \\n{}'.format(classification_report(y_test, y_pred_nmf)))\n",
    "\n",
    "components_df = pd.DataFrame(nmf_knn_cv.best_estimator_.named_steps.nmf.components_, columns=top_50_gene_names)\n",
    "print(components_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
